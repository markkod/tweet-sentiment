{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML sentiment analysis using Naive Bayes model. Dataset: https://github.com/fnielsen/afinn/blob/master/afinn/data/AFINN-111.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, NGram, HashingTF, IDF\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"Naive Bayes\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [ws.strip().split('\\t') for ws in open('tweets.txt')]\n",
    "training_tweets = [(train[i][0], int(train[i + 1][0])) for i in range(0, len(train) - 1, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sqlContext.createDataFrame(training_tweets, [\"sentence\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            sentence|label|\n",
      "+--------------------+-----+\n",
      "|Gas by my house h...|    1|\n",
      "|Theo Walcott is s...|   -1|\n",
      "|its not that I'm ...|   -1|\n",
      "|Iranian general s...|   -1|\n",
      "|with J Davlar 11t...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize tweets\n",
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')\n",
    "countTokens = udf(lambda w: len(w), IntegerType())\n",
    "tokenized = tokenizer.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove basic stopwords\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered')\n",
    "dataset = remover.transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that we don't need in the future\n",
    "dataset = dataset.drop('sentence', 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove URLs, emoticons, hashtags, mentions, RT\n",
    "\n",
    "pattern = re.compile('[\\d|)(:\\(|:\\))+$]')\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    return word.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def is_unwanted_word(word):\n",
    "    if '@' in word:\n",
    "        return True\n",
    "    elif '#' in word:\n",
    "        return True\n",
    "    elif 'http://' in word:\n",
    "        return True\n",
    "    elif 'https://' in word:\n",
    "        return True\n",
    "    elif word == 'RT':\n",
    "        return True\n",
    "    elif pattern.match(word):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def remove_unwanted_values(values):\n",
    "    return [remove_punctuation(x) for x in values if not is_unwanted_word(x)]\n",
    "    \n",
    "\n",
    "map_udf = udf(remove_unwanted_values, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the UDF that replaces punctuation marks and removes mentions, hashtags, links\n",
    "dataset = dataset.withColumn('filtered', map_udf(dataset.filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bigrams\n",
    "ngram = NGram(n=2, inputCol='filtered', outputCol='bigrams')\n",
    "dataset = ngram.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tf-idf\n",
    "hashingTF = HashingTF(inputCol='bigrams', outputCol='TF', numFeatures=20000)\n",
    "tf_df = hashingTF.transform(dataset)\n",
    "\n",
    "idf = IDF(inputCol='TF', outputCol='TF-IDF')\n",
    "idfModel = idf.fit(tf_df)\n",
    "idf_df = idfModel.transform(tf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to sparse vectors, that are needed by the classifer\n",
    "train_dataset = tf_df.rdd.map(lambda row: LabeledPoint(float(row.label), Vectors.fromML(row.TF)))\n",
    "\n",
    "# split data into training and test sets\n",
    "train, test = train_dataset.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model = NaiveBayes.train(train, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy:  0.4731750219876869\n"
     ]
    }
   ],
   "source": [
    "# predict labels on test set\n",
    "pred_label = test.map(lambda x: (x.label, model.predict(x.features)))\n",
    "# calculate accuracy\n",
    "acc = 1.0 * pred_label.filter(lambda x: x[0] == x[1]).count() / test.count()\n",
    "\n",
    "print('Model accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
